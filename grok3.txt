 Welcome to the GROC-3 presentation. So the mission of XAI and GROC is to understand the universe. We're going to understand the nature of the universe. So we can figure out what's going on, where are the aliens, what's the meaning of life, how does the universe end, how does it start, all these fundamental questions were driven by curiosity about the nature of the universe. And that's also what causes us to be a maximally truth-seeking AI. Even if that truth is sometimes at odds with what is politically correct. In order to understand the nature of the universe, you must absolutely rigorously pursue truth, or you will not understand the universe. You'll be suffering from some amount of delusion or error. So that is our goal. Figure out what's going on. And we're very excited to present GROC-3, which is we think an order of magnitude more capable than GROC-2, in a very short period of time. And that's thanks to the hard work of an incredible team. And I'm honored to work with such a great team. And of course, we'd love to have, so the smartest humans out there join our team. So with that, let's go. Hi, everyone. My name is Igor Lead Engineering at XAI. I'm Jimmy Paul, leading research. I'm Tony, working on the reasoning team. All right. Yelena and I don't do anything. I just show up occasionally. Yeah, so like you know, I mentioned GROC is the tool that we're working on. GROC is our AI that we're building here at XAI, and we've been working extremely hard over the last few months to improve GROC as much as we can. So we can give it to all of you. So we can give all of you access to it. We think it's going to be extremely useful. Do you think it's going to be interesting to talk to a really, really funny? And we're going to explain to you how we've improved GROC over the last few months. We've made quite a jump in capabilities. Yeah, actually, we should explain maybe also what is, why do we call it GROC? So GROC is a word from a high-end-level stranger in a strange land. And it's used by a guy who's raised on Mars. And the word GROC is to sort of fully and profoundly understand something. That's what the word GROC means. Fully and profoundly understands them. An empathy is important. True. So yeah, so if we charted XAI's progress in the last few months, it's only been 17 months since we started kicking off our very first model. GROC 1 was almost like a toy by this point, only 214 billion parameters. And now if we proud to progress the time on X-axis, the performance of favorite benchmark numbers at MMOU on the Y-axis, we're literally progressing at an unprecedented speed across the whole field. And then we kick off GROC 1.5 right after GROC 1, release after November 2023, and then GROC 2. So if you look at what all the performance coming from, when you have a very correct engineering team and all the best AI at talent, the only one thing we need is a big intelligence comes from big clusters. So we can reconvert the entire progress of XAI, now replacing the benchmarking Y-axis to the total amount of training flops. That is, how many GPUs we can run at any given time to train our large language models to compress the entire internet? So after GROC 2. Well, all human knowledge, really. That's right. Yeah, internet being part of it, but it's really all human knowledge, everything. Yeah, the whole internet fits into a USB stick at this point. It's like all the human tokens, yeah. That's right. Very soon into the real world. So we have so much trouble actually training GROC 2 back in the days. We kick off the model around February, and we thought we had a large amount of chips, but turned out we can barely get AK training chips running coherently at any given time. And we have so many cooling and power issues. I think you were there in the data center? Yeah, it was like really sort of more like AK chips on average at 80% efficiency, more like 6500 effective H100s training for several months, now we're at 100K. Yeah, that's right. More than 100K. That's right. So what's the next step? Right. So after GROC 2, so if we want to continue to accelerate, we have to take the matter into our own hands. We have to solve all the cooling, all the power issues, and everything. So on April of last year, Elon decided that really the only way for XAI to succeed, for XAI to build the best AI out there is to build our own data center. So we didn't have a lot of time, but because we wanted to give you GROC 3 as quickly as possible, so really we realized we have to build the data center in about four months. It turned out it took us 122 days to get the first 100K GPUs up and running, and there was a monumental effort to be able to do that. It's, we believe it's the biggest, fully connected H100 cluster of its kind. And we didn't just stop there, we actually decided that we need to double the size of the cluster pretty much immediately if we want to build the kind of AI that we want to build. So we then had another phase, which we haven't talked about publicly at CIS. This is the first time that we're talking about this, where we doubled the capacity of the data center yet again, and that one only took us 92 days. So we've been able to use all of these GPUs, use all of this compute to improve GROC in the meantime, and basically today we're going to present the results of that. The fruits that came from that. Yeah, so all the parts, all the rows leads to GROC 3. 10X more compute, more than 10X, really. Yeah, really, maybe 15X. Yep, compared to our previous generation model. And GROC finished the pre-training early January, and it would start, you know, the model still currently training, actually. So this is a little preview of our benchmark numbers. So we evaluated GROC 3 on, you know, three different categories on general mathematical reasoning, general knowledge about STEM and science, and then also on computer science coding. So Amy, American Invitational Math Examination, hosted, you know, once a year. And if we evaluate the model performance, we can see that the GROC 3 across the board is in the league of its own. Even its little brother, GROC 3 Mini, is reaching the frontier across all the other competitors. So you would say, well, at this point, all these benchmarks, you're just evaluating, you know, the memorization of the textbooks, memorization of the GitHub repost, how about the real-time reuse furnace, how about we actually use those models in our product. So what we did instead is, we actually kicked off a blind test of our GROC 3 model, called named Chocolate. Pretty hot. Yeah, hot chocolate. And you know, we've been running on this platform called Cha Ba Arena for two weeks. I think the entire X platform, at some points, speculated, this might be the next generation of AI coming away. So how this Cha Ba Arena works is that it's stripped away the entire product service, right? It just raw comparison of the engine of those APIs, the language models themselves, and placing interface where the user will submit one single query, and you get to show two responses. You don't know which model they come from, and indeed, you make the vote. So in this blind test, GROC 3, an early version of GROC 3, already reached like 1400. No other models had reached an Elon score, had to have comparison to all the other models at this score. And it's not just one single category. It's 1400 aggregated across all the categories. In Cha Ba capabilities, in instruction following, coding, so it's number one across the board in this blind test. And it's still climbing, so we actually keep updating it. So it's 1400, 1400 in climbing. Yeah, and in fact, we have a version of the model that we think is already much better than the one that we tested here. Yeah, we'll see. No, I'll fire gets, but that's the one that we're working on. We're talking about today. Yeah, so actually one thing, if you're using GROC 3, I think you may notice improvements almost every day, because we're continuously improving the model. So literally, even within 24 hours, you'll see improvements. Yep, so, but we believe here at XAI, getting the best pre-training model is not enough. That's not enough to build the best AI. And the best AI need to think like a human. You need to contemplate about all the possible solutions, self-critique, verify all the solutions, backtrack, and also think from the first principle. That's a very important capability. So we believe that as we take the best pre-training model and continue training with reinforcement learning, it will enlist the additional reasoning capabilities that allows the model to become so much better and scale not just in the training time, but actually in the test time as well. So we already found the models extremely useful internally for our own engineering, saving hours of time, hundreds of hours of coding time. So, you're the power user of our growth and reasoning model. So what else do we use cases? Yeah, so like Jimmy said, we've added advanced reasoning, capabilities to GROG, and we've been testing them pretty heavily over the last few weeks. And we're going to give you a little bit of a taste of what it looks like when GROG is solving hard reasoning problems. So we've prepared two little problems for you. One comes from physics, and one is actually a game that GROG is going to write for us. So when it comes to the physics problem, you know, what we want GROG to do is to plot a viable trajectory to do a transfer from Earth to Mars. And then at a later point in time, a transfer back from Mars to Earth. And that requires some physics that GROG will have to understand. So we're going to challenge GROG, you know, come up with a viable trajectory, or calculate it, and then plot it for us. So we can see it. And yeah, this is totally unscripted by the way. That's the entirety of the prompt, which we should be clarifies that there's nothing more than that. Yeah, exactly. This is the GROG interface, and we've typed in this text that you can see here. Generate code for an animated 3D plot of a launch from Earth, landing on Mars, and then back to Earth at the next launch window. And if not kicked off of the query, and you can see GROG is thinking. So a part of GROG's advanced reasoning capabilities are these thinking traces that you can see here. You can even go inside and actually read what GROG is thinking as it's going through the problem, as it's trying to solve it. Yeah. We're saying like, we are doing some obscuration of the thinking so that our model doesn't get totally copied instantly. So there's more to the thinking than is displayed. Yeah. And because this is totally unscripted, there's actually a chance that GROG might make a little coding mistake, and it might not actually work. So just in case we're going to launch two more instances of this. So if something goes wrong, we'll be able to switch to those and show you something that's presented. So we're kicking off the other tool as well. And like I said, we have a second problem as well. And yeah, actually one of the favorite activities here at XVI is having GROG write games for us. And not just any old game, any game that you might already be familiar with, but actually creating new games on the spot and being creative about us. So one example that we found was really, really fun is create a game that's a mixture of the two games, Tetris and Bejoult. This is maybe an important thing. Like obviously if you ask an AI to create a game like Tetris, there are many examples of Tetris on the internet, or you're game like Jule, whatever, it can copy it. What's interesting here is it achieved a creative solution combining the two games that actually works and is a good game. Yeah, but we're seeing the beginnings of creativity. Yeah. Fingers crossed that we can recreate that. Hopefully it works. Hopefully it works. It's actually because this is a bit more challenging, we're going to use something special here which we call big brain. That's our mode in which we use more computation. Which is more reasoning for GROG, just to make sure that there's a good chance here that it actually might actually do it. So we're also going to fire off free attempts here at solving this game. At creating this game, that's a mixture of Tetris and Bejoult. Let's see what GROG comes up. I've played the game. It's pretty good. Like it's like, wow, okay, this is open. Yeah. So while GROG is spanking in the background, we can now actually talk about some concrete numbers, how well is GROG doing across tons of different tasks that we've tested on? So we'll hand it over to Tony to talk about that. Yeah, okay, so let's see how GROG does on those interesting challenging benchmarks. So yeah, so reasoning, again, refers to those models that actually think quite for quite a long time before it tries to solve a problem. So in this case, around the month ago, the GROG3 pre-training finishes. So after that, we've worked very hard to put the reasoning capability into the current GROG3 model. But again, this is very early days, so the model is still currently in training. So right now, what we're going to show to people is this beta version of the GROG3 Alongside, we also are training a mini version of the reasoning model. So essentially, on this plot, you can see the GROG3 reasoning beta and then GROG3 mini reasoning. The GROG3 reasoning is actually a model that we train for much longer time. And you can see that sometimes it actually performs study better compared to the GROG3 reasoning. This also just means that there's a huge potential for the GROG3 reasoning because it's trained for much less time. So let's actually look at how it does on those three benchmarks. So Jimmy also introduced already. So essentially, we're looking at three different areas, mathematics, science, and coding. And for math, we're picking this high school competition problem. For science, we actually picked those PhD-level science questions. And for coding, it's also actually pretty challenging. It's competitive coding and also some legal, which is some code interview problems that people usually get when they interview for companies. So on those benchmarks, you can see that the GROG3 actually performed quite well across the board compared to other competitors. Yeah, so it's pretty promising. These models are very smart. So Tony, what are those shaded bars? Yeah, so okay, so you asked this question. For those models, because it can reason, it can think. You can also ask them to even think longer. You can spend more what we call test and compute, which means you can spend more time to reason to think about a problem before you spit out the answer. So in this case, the shaded bar here means that we just asked the model to spend more time. You can solve the same problem many, many times before it tries to conclude what is the right solution. And once you give this compute or this kind of budget to the model, it turns out the model can even perform better. So this is essentially the shaded bar in those bars. Right, so I think this is really exciting, right? Because now, instead of just doing one chain of thoughts with AI, why not do multiple? For the ones? Yes. So that's a very powerful technique that allows to continue scale the model capabilities after training. And people often ask, are we actually just overfitting to the benchmarks? Yes. So how about your organization? So yes, I think, yeah, this is definitely a question that we are asking ourselves, whether we are overfitting to those current benchmarks. Luckily, we have a real test. So about five days ago, Amy 2025 just finished. This is where high school students compete in this particular benchmark. So we got this very fresh new competition. And then we asked our two models to compete on the same benchmark, at the same exam. And it turns out very interestingly, the graph three reasoning, the big one, actually does better on this particular new fresh exam. This also means that the generalization capability of the big model is stronger, much stronger compared to the smaller model. If you compare to the last year's exam, actually, this is the opposite. The smaller model kind of learns the previous exams better. So yeah, so this actually shows some kind of true generalization from the model. Yeah. So 17 months ago, our GROC zero and GROC one barely solves any high school problems. That's right. And now we have a kid that just already graduated. The GROC is ready to go to college, is that right? Yeah. I mean, it's won't be long before it's simply perfect. The human exams won't be too easy. Yeah, like, and internally, we actually as a GROC continue evolves, we're going to talk about what we excited about. But very soon, there will be no more benchmark left. Yeah. One thing that's quite fascinating, I think is that we basically only trained GROC's reasoning abilities on math problems and competitive coding for. It's a very, very specialized kind of task. But somehow, it's able to work on all kinds of other different tasks. So including creating games, lots and lots of different things. And what seems to be happening is that basically GROC learns the ability to detect its own mistakes and its thinking, correct them, persist on a problem, try lots of different variants, pick the one that's best. So there are these generalized, generalizing abilities that GROC learns from mathematics and from coding, which you can then use to solve all kinds of other problems. So that's, yeah, it's pretty. And reality is the instantiation of mathematics. That's right. And one thing we're actually really excited about that goes back to our founding mission is, what if one day we have a computer just like deep thought that utilizes our entire cluster just for that one very important problem in the test time? All the GROC turned on, right? So I think we'll back then, we'll build in the GPU clusters together. You're applying cables. And I remember that when we turn on the first initial test, you can hear all the GPUs humming in the hallway. That's almost feel like spiritual. Yeah, that's actually a pretty cool thing that we're able to do that we can go into the data center and tinker with the machines there. So for example, we went in and we unplugged, so we're the cables and just made sure that our training setup is still running stable. So that's something that's, you know, I think most AI teams out there don't usually do, but it's actually totally unlocks a new level of reliability and what you're able to do with the hardware. So when are we going to solve Riemann? So the easiest solution is to numerate over all possible strains. And as long as you have a very far enough compute, you'll be able to do it. Okay. My projection will be what you guess. What is your neural edge calculate? So my board prediction, so three years ago, I told you this, I think in two years later, two things are going to happen. We're going to see machines win some medals. Yeah. Two wins award, feels medal, Nobel Prize, with probably some experts in the loop, right? So the expert uplifting. Do you mean, so this year or next year? Oh, oh. That's what it comes down to, really. So it looks like Grock finished all of its thinking on the two problems. So let's take a look at what it said. All right. So this was the little physics problem we had. We've collapsed the thoughts here, so they're hidden. And then we see Grock's answer below that. So it explains, it wrote a Python script here using Matplotlib that gives us all of the code. So let's take a quick look at the code. It seems like it's doing reasonable things here, not totally of the mark. So the capital says here, so maybe it's solving capitalist laws. Capitalist, capitalist law numerically. Yeah, there's really only one way to find out if this thing is working. I'd say let's give it a try. Let's run the code. All right. And we can see Grock is animating two different planets, Earth and Mars here. And then the green ball is the vehicle that's transiting. The spacecraft is transitioning between Earth and Mars. And you could see the journey from Earth to Mars. And it looks like, yeah, indeed, the astronauts were trying safely at the right moment in time. So obviously this was just generated on the spot. So we can't tell you if that was actually correct. Solutions who are going to take a closer look, maybe we're going to call some colleagues from space X, ask them if this is legit. That's pretty close. It's, I mean, there's a lot of complexities in the actual orbits that actually take the entire cap. But this is pretty close to what it looks like. Awesome. In fact, out of that are my pendants here. This is about the Earth Mars home and transfer on it. Yeah. When are we going to install Grock on a rocket? Well, I suppose in two years. Three years? Everything is two years away. Well, Earth and Mars transit can occurs every 26 months. The next, we're currently in a transit window approximately. The next one would be November of next year, roughly the end of next year. And before we go as well, space X will send Starship rockets to Mars and with the Optimus robots and Grock. I'm curious what this combination of Tetris and the Jules looks like, the Tetris, as we've named it internally. So, okay, we also have an output from Grock here. It's has a butterfly script, spains that is what it's been doing. If you look at the code, there are some constants that are being defined here, some colors, or then the terminals, the pieces of Tetris are there. Obviously, very hard to see at one glance, if this is good, so we've got to run this to figure out if it's working. Well, let's give it a try. The fingers crossed. All right, all right. So, this kind of looks like Tetris. But the colors are a little bit off. Right, the colors are different here. And if you think about what's going on here, the Jules has this mechanic where you if you get three Jules in a row, then they disappear and also gravity activates. Right, so what happens if you get three of the colors together? Oh, yeah, so something happened. So, I think what Grock did in this version is that once you connect at least three blocks of the same color, in a row, then gravity activates and they disappear and then gravity activates and all the other blocks fall down. I kind of curious if there's still a Tetris mechanic here, where if the line is full, does it actually clear it or what happens then? It's up to interpretation now, so who knows? Yeah, I mean, when you see it, it'll do different variants when you ask it. It doesn't do the same thing every time. Exactly. We've seen a few other Tetris that worked very differently, but the song seems full, so. Yeah. Are we ready for Game Studio at X.Doll AI? Yes. So we're launching an AI gaming studio at X.D.I. if you're interested in joining us and building AI games, please join X.D.I. where we're launching an AI gaming studio, we're announcing it tonight. Let's go. Epic games. But all right, that's an actual game. Yeah. Yeah. All right, so I think one thing is super exciting for us is that once you have the bass pre-train model, you have the bass-reason model, right? So we already see that when you actually give the capability for those models to think harder, think longer, think more broad, the performance continues proves. And we're really excited about the next frontier that will happen if we're not only allowed to model the thing harder, but also provide more tools. It's like how real humans to solve those problems. For real humans, we're only asking to solve RIMIN hypothesis just with a piece of pen and paper going into that. So with all the basic web browsing, search engine, and coding interpreters, that builds the foundations and the bass-reason model, builds the foundations for the GROG agent to come. So today, we're actually introducing a new product called DeepSearch. That is the first generation of our GROG agents that not just helping the engineers and research and scientists to do coding, but actually help everyone to answer questions that you have today. It's a kind of like a next generation search engine that really helps you to understand the universe. So you can start asking questions like, for example, hey, when is the next starship launch day? For example, so let's try that if we hit the answer. The left-hand side, we see a high-level progress bar, essentially, the model knowledge is going to do one single search like the current RIC system, but actually thought very deeply about, hey, what's the user intent here? And what are the facts actually consider at the same time? And how many different websites actually actually go and read their content, right? So this can really save hundreds of hours of everyone's Google time if you want to really look into certain topics. And then on the right-hand side, you can see the bullet summary of how the current model is doing, what websites, browsing, what sources verify, and oftentimes actually cross validate different sources out there to make sure the answer is actually correct before it's output final answer. And we can at the same time fire up a few more queries. How about you don't use a gamer, right? So what are some of the best builds, and most popular builds in the Path of Excel, hardcore, right? How hardcore, the... If you can't technically just look at the hardcore ladder. It might be a fast way to figure it out. Yeah, we're seeing what the model does. And then we can also do something more fun, for example, how about like make a prediction about the marsh madness out there? Yeah, so this is kind of a fun one where Warren Buffett has a billion dollar bet. If you can exactly match the, I think the, the sort of the entire winning tree of marsh madness, you can win a billion dollars from Warren Buffett. So like, would we pretty cool if AI could help you win at billion dollars from Buffett? That seems like a pretty good investment. Let's go. Yeah. All right, so now let's fire up the query and see what model it does. So we can actually go back to our very first one. How about the... Buffett wasn't counting on this. Sorry, that's right. Okay, so we got the result of the first one and model thought around one minute. So okay, so the key inside here, the next partnership is gonna be on 24th or later. So no earlier than February 24th. It might be sooner. Okay. So yeah, so I think we can, you know, grow down, go down what the model does. So it does little research on the flight seven, what happened, got grounded, and actually it looked into the FCC filing, you know, from its data collections. And then actually made a new conclusion that, yeah, if we continue to grow down, right. Yeah, so it makes the little table, I think inside XAI, we're often joked about the time to the first table is the only latency that matters. Yeah, so that's how the model making inference and look at all the sources. And then we can look into the gaming one. So how about the, right. So for this particular one, we look at, hey, the, you know, the building is light and, yeah, it's kind of more better. Yeah. So when the infernal is, but if we go down, so the surprising fact of all the other builds, so it looked into the 12 classes. Yeah, so we'll see that the minimum build was pretty popular when the game first came out and now the, the invoker is of the world, yeah, took over invoker, monkey vocal for sure. Yeah, that's right. Yeah, followed by the stone weavers then that's really good at mapping. So yeah, and we can see the match manners. How about that? So what, what you should think about the deep search is that, if we actually go into the panel where shows, you know, what are the sub tasks, you can actually click the bottom left of this, right. And then in this case, you can actually scroll through, actually reading to the mind of GROC. What information does the model actually think about or track was the what or not? How does it actually cross-value different information sources? So that makes the entire search experience that information retrieval process a lot more transparent to our users. And this is much more powerful than any search engine out there. You can literally just tell it, only use sources from X, you know, or try to respect that. Yeah. And so it's much more steerable, much more intelligent. I mean, it really should save you a lot of time. So something that might take you half an hour or an hour of researching on the web or searching social media, you can just ask it to go do that and come back in 10 minutes later. It's done an hour worth of work for you. That's really what it comes down to. Exactly. And maybe better than you could have done it yourself. Yeah. Think about you have info-mount of interns working for you. Now you can just fire up all the tasks and come back a minute later. So this is going to be interesting one. So, a match manners had not happened yet. So I guess we have to follow up with a next livestream. Yeah. It seems like pretty good. $40 might get you a billion dollars. $40 subscription. That's right. I mean, my work. So, yeah. So when are the users going to have their hands on GROG3? Yeah. So the great news is we've been working tirelessly to actually release all of these features that we've shown you. The GROG3 base model with amazing chat capabilities. That's really useful. That's really interesting to talk to. The deep search, the advanced reasoning mode, all of these things. We want to vote them out to you today, starting with the premium plus subscribers on X. So it's the first group that will initially get access. Make sure to update your X app if you want to see all of the advanced capabilities, because we just released the update now as we're talking here. And yeah, if you're interested in getting early access to GROG, then sign up for premium plus. And also we're announcing that we're starting a separate subscription for GROG that we call SuperGROG for those real GROG fans that want the most advanced capabilities and the earliest access to new features. So feel free to check that out as well. This is for the dedicated GROG app and for the website. The exact same. So our new website is called GROG.com. And you'll also find. You never guess. Yeah, you never guess. And you can also find our GROG app in the iOS app store. And that gives you even more polished experience that's totally GROG focused. If you want to have GROG, you're easily available one tap away. Yeah. And the version on GROG.com on a web browser is going to be the latest and most advanced version, because obviously it takes us a while to get something into an app and they get it approved by the app store. And if that's something on a phone format, there's limitations what you can do. So the most powerful version of GROG and the latest version will be the web version at GROG.com. So watch out for the name GROG-free in the app. Did give away. Exactly. That's the giveaway that you have GROG-free. And if it says GROG-free, then it GROG-free hasn't quite arrived for yet. But we're working hard to roll this out today and then to even more people over the coming days. Yeah. Make sure you update your phone app too, where you actually get all the tools we showcase today with the thinking mode, with the deep search. So yeah, really looking forward to all the feedbacks you have. Yeah. I think we should emphasize that this is kind of a beta, like meaning that you should expect some imperfections at first. But we will improve it rapidly almost every day. In fact, every day, I think it'll get better. So if you want a more polished version, I'd like maybe way to week, but expect improvements literally every day. And then we're also going to be providing a voice interaction. So you can have conversational. In fact, I was trying it earlier today. It's working pretty well, but these are a bit more polished. The sort of way we can just literally talk to it like you're talking to a person. It's that's awesome. It's actually, I think, one of the best experiments of GROG. But that's probably about a week away. Yeah. So what's our set? I think we might have some audience questions. Sure. Yeah, right? All right. Let's take a look. Yeah, let's take a look. The audience from the Ask Platform. Yeah. So the first question here is, when GROG voice assistance? When is it coming out? As soon as possible, just like Elon said, just a little bit of polishing away from being reached to everybody, obviously, it's going to be released in an early form, and we're going to rapidly iterate on that. And the next question is, when will GROG3 be in the API? So this is coming in the GROG3 API with both the reasoning models and deep search is coming away in the coming weeks. We're actually very excited about the enterprise use cases of all these additional tools that GROG has access to, at how the test-time compute and tool use can actually really accelerate all the business use cases. Another one is, will voice mode be native or text to speech? So I think that means is it going to be one model that is understanding what you say and then talking back to you or is it going to be some system that has text to speech? Inside of it, in the good news is it's going to be one model, like a variant of GROG3 that we're going to release, which basically understands what you're saying, and then generates the audio directly from that. So very much like GROG3 generates text, that model generates audio. And that has a bunch of advantages. I was talking to it earlier today and it said, hi, IGROG, reading my name from probably from some text that it had. And I said, no, my name is Igor. And it remembered that. So it could continue to say Igor, just like a human word. And you can't achieve that with Texas speech. So here's a question for you. Pretty spicy. Elon, is GROG a boy or a girl? And how they sing? GROG is whatever you want it to be. Yeah. Yeah. Audio single? Yes. All right. The shop is open. So I don't see people are going to fall in love with GROG. It's like 1,000% probable. The next question, will GROG be able to transcribe audio into text? Yes. So we'll have this capability in both the app and also the API. So we thought that GROG should just be your personal assistant, looking on your shoulder and follow you along the way. Learn everything you have learned and really help here to understand the world better, become smarter every day. Yeah. The voice metagrog doesn't simply, it's not just voice to text, it understands tone, inflection, pacing, everything as well. I mean, it's like talking to a person. Yup. So any plans for conversation memory? Yeah. Absolutely. We're working on it right now. I'm not really forgot. That's right. Let's see. What are the other ones? So what about the DM features? So if you have personalizations, and if you have GROG remembers your previous interactions, should it be one GROG or multiple different GROGs? It's up to you. You can have one GROG or many GROGs. I suspect people will probably have more than one. Yeah. I won't have a doctor GROG. Yeah. The GROG dog. That's right. Cool. So in the past, we've open-source GROG1. So somebody is asking us, are we going to do it again? What GROG2? Yeah. I think that general approach is that we will open-source the last version when the next version is fully out. Like when GROG3 is mature and stable, which is probably within a few months, then we'll open-source GROG2. OK. So we'll probably have time for our last question. What was the most difficult part about working on this project? I assume GROG3 and what I'm most excited about. So I think me looking back, getting the whole model training on 100K H100 coherently, that's almost like battling against the final boss of the universe, the entropy. Because at any given time, you can have a cosmic ray that beaming down and flip a bit in your transistor and now the entire gradient update, if it's a piece of it, the entire gradient update is out of whack. And now you have 100,000 of those. And you have to orchestrate them at any given time. Any of the GPUs can go down. Yeah. I mean, it's worth breaking down. How were we able to get the world's most powerful training cluster operational within 122 days? Because we started off. We actually weren't intending to do a data center ourselves. We went to the data center providers and said how long would it take to have 100,000 GPUs operating coherently in a single location. And we got time frames from 18 to 24 months. So we're like, well, 18 to 24 months, that means losing as a certainty. So the only option was to do it ourselves. So if you break down the problem, guess I'm doing like reasoning here. Makes you think. One single chain though. Yeah, exactly. So what we needed a building, we can't build a building. So we must use an existing building. So we looked for basically for factories that had been abandoned, but the factory was in good shape, like the company had gone bankrupt to something. So we found an electrolux factory in Memphis. That's why it's in Memphis, home of Elvis. And also one of the oldest, I think, was the capital of ancient Egypt. And it was actually very nice factory that, for whatever reason, that electrolux had left. And that gave us shelter for the computers. Then we needed power. We needed at least 120 megawatts at first, but the building only had 15 megawatts. And ultimately, for 200,000 megawatts, 200,000 GPUs, we needed a quarter gigawatt. So we initially, at least a whole bunch of generators. So we have generators on one side of the building. Just trailer after trailer of generators, until we can get the utility power to come in. And then we also need cooling. So on the other side of the building, it was just trailer after trailer of cooling. So we released about a quarter of the mobile cooling capacity of the United States on the other side of the building. Then we needed to get the GPUs all installed. And they're all liquid cooled. So in order to achieve the density necessary, this is a liquid cooled system. So we had to get all the plumbing for the liquid cooling. Now we did ever done a liquid cooling data center at scale. So this was an incredibly dedicated effort by a very talented team to achieve that outcome. And I may think now it's going to work. Nope. The issue is that the power fluctuations for GPU cluster are dramatic. So it's like this giant symphony that is taking place. Like a symphony with 100,000 or 200,000 participants in the symphony, and the whole orchestra will go quiet and loud in 100 milliseconds. And so this course, massive power fluctuation. So then which then caused the generators to lose their minds and they weren't expecting this. So to buffer the power, we then used Tesla mega packs to smooth out the power. So the mega packs had to be reprogrammed. So with the XAI, we were working with Tesla. We reprogrammed the mega packs to be able to deal with these dramatic power fluctuations. To smooth out the power, the computers could actually run properly. And that worked quite tricky. And then, but even at that point, you just left to make the computers all communicative. So all the networking had to be solved. And a debugging of zillion network cables, a debugging nickel at four in the morning. Or like we solved it like four. What party can? Roughly 4.20 AM. Yes, thank you. We figured out like there's some, well, there were a whole bunch of issues. We're like one, there was like a bios mismatch. bios was not set up correctly. We had to have less PCI outputs between two different machines. One that was working. Yeah. One that was not worked. Many, many other things. So that means that exactly this would go on for a long time if we actually solved things. But there's like interesting. It's not like, oh, we just magically made it happen. You had to break down the problem just like GROC does for reasoning into the constituent elements and then solve each of the constituent elements in order to achieve a coherent training cluster in a period of time that is a small fraction of what anyone else could do it in. And then once the training cluster was up and running and we could use it, now we have to make sure that it actually stays healthy throughout, which is on giant challenge. And then we had to get every single detail of the training right in order to get a GROC-free level model, which is actually really, really hard. So we don't know if there are any other models out there that have GROC-free scababities. But whoever trains a model better than GROC-free has to be extremely good at the science of deep learning at every aspect of the engineering. So it's not so easy to pull this off. And this now going to be the last cluster we've built and last model we trained. Oh, yeah. We've already started work on the next cluster, which will be about five times the power. So instead of quarter-gagot, roughly 1.2, gagot, what's the back to the future was? What's the power? Does the back to the future car? Yeah. Don't know. Anyway, the back to the future power. It's just roughly in that order, I think. So these will be the GB200 slash 300 cluster. It won't be the most powerful training cluster in the world. So we're not stopping here. And our reason model is going to continue and improve by accessing more tools every day. So we're very excited to share any of the upcoming results with you all. Yeah, the thing that keeps us going is basically being able to give Ggok free to you and then seeing the usage go up, seeing everybody enjoy. No, Ggok, that's what really gets us up in the morning. So yeah. Yeah, thanks for tuning in. Awesome. Thanks, guys. Hey, Dr. Kuzak. Can you hear me? I'm so excited to finally meet you. I can't wait to chat and learn more about each other. I'll talk to you soon.